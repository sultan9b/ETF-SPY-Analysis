import logging

import duckdb
import pendulum
from airflow import DAG
from airflow.models import Variable
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.external_task import ExternalTaskSensor

# ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ DAG
OWNER = "sonador"
DAG_ID = "raw_unemployment_from_s3_to_pg"

# Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ñ‹Ğµ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹ Ğ² DAG
LAYER = "raw"
SOURCE = "unemployment_owid"
SCHEMA = "ods"
TARGET_TABLE = "fct_unemployment"

# S3 / MinIO
ACCESS_KEY = Variable.get("access_key")
SECRET_KEY = Variable.get("secret_key")

# PostgreSQL
PASSWORD = Variable.get("pg_password")

LONG_DESCRIPTION = """
# Unemployment Data to PostgreSQL Pipeline

Ğ­Ñ‚Ğ¾Ñ‚ DAG Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¾Ğ± ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ¸Ğ· MinIO/S3 
Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸Ñ… Ğ² PostgreSQL DWH.

## Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: MinIO bucket prod
- ĞŸÑƒÑ‚ÑŒ: raw/unemployment_owid/YYYY-MM-DD/*.parquet
- Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Parquet (ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ GZIP)

## Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°
- Ğ‘Ğ°Ğ·Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: postgres_dwh
- Ğ¡Ñ…ĞµĞ¼Ğ°: ods
- Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°: fct_unemployment

## ĞŸĞ¾Ğ»Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹
- load_date: Ğ´Ğ°Ñ‚Ğ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸
- entity: Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹
- code: ĞºĞ¾Ğ´ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹ (ISO)
- year: Ğ³Ğ¾Ğ´
- unemployment_rate: ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ (%)

## Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
- ĞĞ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ DAG: raw_unemployment_from_owid_to_s3
"""

SHORT_DESCRIPTION = "Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¾Ğ± ÑƒÑ€Ğ¾Ğ²Ğ½Ğµ Ğ±ĞµĞ·Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ¸Ñ†Ñ‹ Ğ¸Ğ· S3 Ğ² PostgreSQL DWH"

args = {
    "owner": OWNER,
    "start_date": pendulum.datetime(2025, 1, 1, tz="Asia/Almaty"),
    "catchup": False,
    "retries": 3,
    "retry_delay": pendulum.duration(hours=1),
}


def get_dates(**context) -> tuple[str, str]:
    """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ñ‚Ñ‹ Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ° Airflow"""
    start_date = context["data_interval_start"].format("YYYY-MM-DD")
    end_date = context["data_interval_end"].format("YYYY-MM-DD")
    return start_date, end_date


def create_table_if_not_exists(**context):
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ² PostgreSQL ĞµÑĞ»Ğ¸ ĞµÑ‘ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
    """

    logging.info("ğŸ”§ Creating table if not exists...")
    con = duckdb.connect()

    try:
        con.sql(f"""
            CREATE SECRET dwh_postgres (
                TYPE postgres,
                HOST 'postgres_dwh',
                PORT 5432,
                DATABASE postgres,
                USER 'postgres',
                PASSWORD '{PASSWORD}'
            );

            ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);

            CREATE SCHEMA IF NOT EXISTS dwh_postgres_db.{SCHEMA};

            CREATE TABLE IF NOT EXISTS dwh_postgres_db.{SCHEMA}.{TARGET_TABLE} (
                load_date DATE NOT NULL,
                entity VARCHAR(255) NOT NULL,
                code VARCHAR(10),
                year INTEGER NOT NULL,
                unemployment_rate DECIMAL(10,6),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (entity, year, load_date)
            );
        """)

        logging.info(f"âœ… Table {SCHEMA}.{TARGET_TABLE} is ready")

    except Exception as e:
        logging.error(f"âŒ Error creating table: {str(e)}")
        raise
    finally:
        con.close()


def get_and_transfer_raw_data_to_ods_pg(**context):
    """
    Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· MinIO/S3 Ğ² PostgreSQL DWH
    """

    start_date, end_date = get_dates(**context)
    logging.info(f"ğŸ’» Start load for dates: {start_date}/{end_date}")

    con = duckdb.connect()

    try:
        # ĞŸÑƒÑ‚ÑŒ Ğº Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ² S3
        s3_path = f"s3://prod/{LAYER}/{SOURCE}/{start_date}/{start_date}_00-00-00.gz.parquet"

        logging.info(f"ğŸ“¥ Reading data from: {s3_path}")

        con.sql(f"""
            SET TIMEZONE='UTC';
            INSTALL httpfs;
            LOAD httpfs;
            SET s3_url_style = 'path';
            SET s3_endpoint = 'minio:9000';
            SET s3_access_key_id = '{ACCESS_KEY}';
            SET s3_secret_access_key = '{SECRET_KEY}';
            SET s3_use_ssl = FALSE;

            CREATE SECRET dwh_postgres (
                TYPE postgres,
                HOST 'postgres_dwh',
                PORT 5432,
                DATABASE postgres,
                USER 'postgres',
                PASSWORD '{PASSWORD}'
            );

            ATTACH '' AS dwh_postgres_db (TYPE postgres, SECRET dwh_postgres);
        """)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ² Ñ„Ğ°Ğ¹Ğ»Ğµ
        count_result = con.sql(f"""
            SELECT COUNT(*) as total_rows
            FROM '{s3_path}'
        """).fetchone()

        logging.info(f"ğŸ“Š Found {count_result[0]} rows in source file")

        # Ğ£Ğ´Ğ°Ğ»Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ñ€Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ·Ğ° ÑÑ‚Ñƒ Ğ´Ğ°Ñ‚Ñƒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
        con.sql(f"""
            DELETE FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
            WHERE load_date = '{start_date}';
        """)

        logging.info(f"ğŸ—‘ï¸ Deleted old data for date: {start_date}")

        # Ğ’ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
        con.sql(f"""
            INSERT INTO dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
            (
                load_date,
                entity,
                code,
                year,
                unemployment_rate
            )
            SELECT
                load_date,
                Entity as entity,
                Code as code,
                Year as year,
                "Unemployment, total (% of total labor force) (modeled ILO estimate)" as unemployment_rate
            FROM '{s3_path}'
            WHERE Year IS NOT NULL;
        """)

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€Ğ¸Ñ‚ÑŒ ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ²ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¾
        inserted_count = con.sql(f"""
            SELECT COUNT(*) as inserted_rows
            FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
            WHERE load_date = '{start_date}';
        """).fetchone()

        logging.info(f"âœ… Inserted {inserted_count[0]} rows into PostgreSQL")

        # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ğ½Ğ°Ğ¼
        stats = con.sql(f"""
            SELECT 
                entity,
                COUNT(*) as years_count,
                MIN(year) as min_year,
                MAX(year) as max_year,
                ROUND(AVG(unemployment_rate), 2) as avg_rate
            FROM dwh_postgres_db.{SCHEMA}.{TARGET_TABLE}
            WHERE load_date = '{start_date}'
            GROUP BY entity
            ORDER BY entity;
        """).fetchall()

        logging.info(f"ğŸ“ˆ Statistics by country:")
        for row in stats:
            logging.info(f"   {row[0]}: {row[1]} years ({row[2]}-{row[3]}), avg: {row[4]}%")

    except Exception as e:
        logging.error(f"âŒ Error transferring data: {str(e)}")
        raise
    finally:
        con.close()

    logging.info(f"âœ… Transfer for date success: {start_date}")


# ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ DAG
with DAG(
        dag_id=DAG_ID,
        schedule_interval="@monthly",  # Ğ Ğ°Ğ· Ğ² Ğ¼ĞµÑÑÑ†, ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾ Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ¸Ğ· OWID
        default_args=args,
        tags=["s3", "ods", "pg", "unemployment", "dwh"],
        description=SHORT_DESCRIPTION,
        concurrency=1,
        max_active_tasks=1,
        max_active_runs=1,
) as dag:
    dag.doc_md = LONG_DESCRIPTION

    start = EmptyOperator(
        task_id="start",
    )

    # Ğ¡ĞµĞ½ÑĞ¾Ñ€ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµÑ‚ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ñ DAG Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ· OWID
    sensor_on_raw_layer = ExternalTaskSensor(
        task_id="sensor_on_raw_layer",
        external_dag_id="raw_unemployment_from_owid_to_s3",
        external_task_id="end",  # ĞĞ¶Ğ¸Ğ´Ğ°Ñ‚ÑŒ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ "end"
        allowed_states=["success"],
        mode="reschedule",
        timeout=3600,  # 1 Ñ‡Ğ°Ñ
        poke_interval=60,  # Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¼Ğ¸Ğ½ÑƒÑ‚Ñƒ
    )

    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ ĞµÑĞ»Ğ¸ Ğ½Ğµ ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚
    create_table_task = PythonOperator(
        task_id="create_table_if_not_exists",
        python_callable=create_table_if_not_exists,
    )

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· S3 Ğ² PostgreSQL
    transfer_data_task = PythonOperator(
        task_id="get_and_transfer_raw_data_to_ods_pg",
        python_callable=get_and_transfer_raw_data_to_ods_pg,
    )

    end = EmptyOperator(
        task_id="end",
    )

    start >> sensor_on_raw_layer >> create_table_task >> transfer_data_task >> end